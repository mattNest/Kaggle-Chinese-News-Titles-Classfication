{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Embedding, CuDNNGRU\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1545632846951837664\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15570885896676600047\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12870993957629875859\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 29525491712\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10950793084406069784\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-DGXS-32GB, pci bus id: 0000:07:00.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropcomma(sent):\n",
    "    if isinstance(sent, str):\n",
    "        return sent.replace(',', '')\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  whether or not use key_words in dataset(special for this toutiao dateset)\n",
    "def add_keywords(x):\n",
    "    if True:\n",
    "        return np.array([dropcomma(xi) for xi in x])\n",
    "    else:\n",
    "        return np.full_like(x, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_aug = pd.read_csv('./data/train_data_preprocessed_with_NA_aug_40000.csv')\n",
    "df_test = pd.read_csv('./data/test_data.csv') \n",
    "df_train_all = pd.read_csv('./data/df_train_all.csv')\n",
    "#print(len(df_train_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_train_all['title'].values\n",
    "key_words = df_train_all['keyword'].values\n",
    "labels = df_train_all['label'].values\n",
    "\n",
    "sentences_t = df_test['title'].values\n",
    "key_words_t = df_test['keyword'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457921\n",
      "457921\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words_drop_comma = add_keywords(key_words)\n",
    "key_words_drop_comma_t = add_keywords(key_words_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = [\n",
    "    sentence + keyword for sentence, keyword in zip(sentences,key_words_drop_comma) \n",
    "    if sentence is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = [\n",
    "    sentence + keyword for sentence, keyword in zip(sentences_t,key_words_drop_comma_t) \n",
    "    if sentence is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_all = sentences_train + sentences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517829"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_list(sentences):\n",
    "    sent_list = []\n",
    "    flag_list = ['n','ng','nr','nrfg','nrt','ns','nt','nz','s','j','l','i','v','vn','eng']\n",
    "    \n",
    "    for num in range(len(sentences)):\n",
    "        \n",
    "        if num % 10000 == 0:\n",
    "            print(\"No. %d\" % (num))\n",
    "        \n",
    "        sent = sentences[num]\n",
    "        sent = pseg.cut(sent) # get part of speech\n",
    "\n",
    "        tmp_list = []\n",
    "        for word, flag in sent:\n",
    "            if flag in flag_list:\n",
    "                tmp_list.append(word)\n",
    "\n",
    "        sent_list.append(tmp_list)\n",
    "    \n",
    "    \n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_list_train = get_sent_list(sentences_train)\n",
    "# sent_list_test = get_sent_list(sentences_test)\n",
    "# sent_list_all = get_sent_list(sentences_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(sent_list_train))\n",
    "# print(len(sent_list_test))\n",
    "# print(len(sent_list_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/sents_test.txt', 'w') as f:\n",
    "#     for item in sent_list_test:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_texts = open('./data/sents_all.txt').read().split('\\n')\n",
    "train_texts = open('./data/sents_train.txt').read().split('\\n')\n",
    "test_texts = open('./data/sents_test.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(train_and_test_texts, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 177537 unique tokens.\n",
      "Shape of training data tensor: (457921, 300)\n",
      "Shape of label tensor: (457921, 10)\n",
      "Shape of testing data tensor: (59908, 300)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "tokenizer_train = Tokenizer()\n",
    "tokenizer_test = Tokenizer()\n",
    "tokenizer_all = Tokenizer()\n",
    "\n",
    "tokenizer_train.fit_on_texts(train_texts[:-1])\n",
    "tokenizer_test.fit_on_texts(test_texts[:-1])\n",
    "tokenizer_all.fit_on_texts(train_and_test_texts[:-1])\n",
    "\n",
    "sequences_train = tokenizer_train.texts_to_sequences(train_texts[:-1]) # only training sentences\n",
    "sequences_test = tokenizer_train.texts_to_sequences(test_texts[:-1]) # only testing sentences\n",
    "sequences_all = tokenizer_all.texts_to_sequences(train_and_test_texts[:-1]) # training + testing\n",
    "\n",
    "word_index = tokenizer_all.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "training_data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "testing_data = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "print('Shape of training data tensor:', training_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Shape of testing data tensor:', testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    training_data, labels, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Using Own Trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 232)          41188816  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               1525760   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 42,719,706\n",
      "Trainable params: 42,719,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# original model\n",
    "EMBEDDING_DIM = 232\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, \n",
    "          input_length=MAX_SEQUENCE_LENGTH))\n",
    "model_1.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './models/lstm_add_back_epoch_10_model_4.h5'\n",
    "# model_1 = load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "457921/457921 [==============================] - 326s 712us/step - loss: 0.4036 - acc: 0.8934\n",
      "Epoch 2/10\n",
      "457921/457921 [==============================] - 322s 703us/step - loss: 0.1554 - acc: 0.9558\n",
      "Epoch 3/10\n",
      "457921/457921 [==============================] - 330s 722us/step - loss: 0.1234 - acc: 0.9651\n",
      "Epoch 4/10\n",
      "457921/457921 [==============================] - 325s 709us/step - loss: 0.1020 - acc: 0.9715\n",
      "Epoch 5/10\n",
      "457921/457921 [==============================] - 329s 718us/step - loss: 0.0920 - acc: 0.9747\n",
      "Epoch 6/10\n",
      "457921/457921 [==============================] - 324s 709us/step - loss: 0.0759 - acc: 0.9792\n",
      "Epoch 7/10\n",
      "457921/457921 [==============================] - 322s 703us/step - loss: 0.0703 - acc: 0.9809\n",
      "Epoch 8/10\n",
      "457921/457921 [==============================] - 324s 707us/step - loss: 0.0587 - acc: 0.9840\n",
      "Epoch 9/10\n",
      "457921/457921 [==============================] - 325s 710us/step - loss: 0.0528 - acc: 0.9856\n",
      "Epoch 10/10\n",
      "457921/457921 [==============================] - 324s 708us/step - loss: 0.0472 - acc: 0.9871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff13d1ac780>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='rmsprop',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "print(model_1.metrics_names)\n",
    "\n",
    "model_1.fit(training_data, labels, \n",
    "            epochs=10, batch_size=512) # full data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './models/lstm_add_back_epoch_11_model_4.h5'\n",
    "# model_1.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59908/59908 [==============================] - 10s 168us/step\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "pred_list_ans = []\n",
    "\n",
    "pred_list = model_1.predict(testing_data, verbose=1, batch_size=1024)\n",
    "pred_list_ans = [pred_list[num].argmax() for num in range(len(pred_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = df_test['id'].values\n",
    "df_ans = pd.DataFrame(data=pred_list_ans, index=id_list, columns=['label'])\n",
    "# df_ans.to_csv('./results/lstm_add_back_epoch_11_model_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Using Pretrained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1348401 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('./data/merge_sgns_bigram_char300.txt') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "MAX_NUM_WORDS = 1348401\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 300)          53261400  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 53,834,338\n",
      "Trainable params: 572,938\n",
      "Non-trainable params: 53,261,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM with no word2vec\n",
    "\n",
    "model_wv = Sequential()\n",
    "model_wv.add(embedding_layer)\n",
    "model_wv.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_wv.add(Dropout(0.2))\n",
    "model_wv.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model_wv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv = load_model('./models/lstm_wv2_add_back_epochs_20_full_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "Epoch 1/4\n",
      "457921/457921 [==============================] - 177s 387us/step - loss: 0.1898 - acc: 0.9402\n",
      "Epoch 2/4\n",
      "457921/457921 [==============================] - 177s 387us/step - loss: 0.1928 - acc: 0.9387\n",
      "Epoch 3/4\n",
      "457921/457921 [==============================] - 176s 384us/step - loss: 0.1826 - acc: 0.9422\n",
      "Epoch 4/4\n",
      "457921/457921 [==============================] - 177s 387us/step - loss: 0.1803 - acc: 0.9430\n"
     ]
    }
   ],
   "source": [
    "model_wv.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='rmsprop',\n",
    "                 metrics=['acc'])\n",
    "\n",
    "print(model_wv.metrics_names)\n",
    "\n",
    "model_wv.fit(training_data, labels, epochs = 4, batch_size = 1024)\n",
    "\n",
    "model_wv.save('./models/lstm_wv2_add_back_epochs_24_full_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59908/59908 [==============================] - 7s 119us/step\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "pred_list_ans = []\n",
    "pred_list = model_wv.predict(testing_data, verbose = 1, batch_size = 1024)\n",
    "pred_list_ans = [pred_list[num].argmax() for num in range(len(pred_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = df_test['id'].values\n",
    "df_ans = pd.DataFrame(data=pred_list_ans, index=id_list, columns=['label'])\n",
    "df_ans.to_csv('./results/lstm_wv2_add_back_epochs_24_full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
